{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPISODE = 1000\n",
    "\n",
    "N_HEIGHT = 7 # grid height\n",
    "N_WIDTH = 7 # grid width "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridEnv(object):\n",
    "    def __init__(self):\n",
    "        self.height = N_HEIGHT\n",
    "        self.width = N_WIDTH\n",
    "\n",
    "        self.action_dict = {\"up\":0, \"right\": 1, \"down\": 2, \"left\": 3}\n",
    "        self.action_coords = np.array([[-1,0], [0,1], [1,0], [0,-1]], dtype=np.int)\n",
    "        self.num_actions = len(self.action_dict.keys())\n",
    "\n",
    "        self.state_dim = (self.height, self.width)\n",
    "        self.action_dim = (self.num_actions,)\n",
    "        self.state_action_dim = self.state_dim + self.action_dim\n",
    "\n",
    "        self.obstacles = []\n",
    "        self.add_obstacle(6, 5)\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def add_obstacle(self, h, w):\n",
    "        self.obstacles.append([h, w])\n",
    "\n",
    "    def get_valid_actions(self):\n",
    "        actions = []\n",
    "        h = self.current_state[0]\n",
    "        w = self.current_state[1]\n",
    "        if (h > 0): actions.append(self.action_dict[\"up\"])\n",
    "        if (h < self.height-1): actions.append(self.action_dict[\"down\"])\n",
    "        if (w > 0): actions.append(self.action_dict[\"left\"])\n",
    "        if (w < self.width-1): actions.append(self.action_dict[\"right\"])\n",
    "        actions = np.array(actions, dtype=np.int)\n",
    "        return actions\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_state = np.array([0, 0], dtype=np.int)\n",
    "        return self.current_state\n",
    "\n",
    "    def step(self, action):\n",
    "        self.current_state = np.add(self.current_state, self.action_coords[action])\n",
    "        if np.array_equal(self.current_state, [self.height-1, self.width-1]):\n",
    "            reward = 100\n",
    "            done = True\n",
    "        elif list(self.current_state) in self.obstacles:\n",
    "            reward = -10\n",
    "            done = False\n",
    "        else:\n",
    "            reward = -1\n",
    "            done = False\n",
    "        return self.current_state, reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "\n",
    "        self.learning_rate = 0.1\n",
    "        self.discount_factor = 0.99 \n",
    "        self.epsilon = 0.9\n",
    "\n",
    "        self.q_table = np.zeros(env.state_action_dim, dtype=np.float)\n",
    "\n",
    "    def update_table(self, state, action, reward, next_state):\n",
    "        q_prev = self.q_table[state[0], state[1], action]\n",
    "        q_target = reward + self.discount_factor * max(self.q_table[next_state[0], next_state[1]])\n",
    "        self.q_table[state[0], state[1], action] += self.learning_rate * (q_target - q_prev)\n",
    "\n",
    "    def get_action(self, state, greedy=False):\n",
    "        if greedy:\n",
    "            epsilon = 0\n",
    "        else:\n",
    "            epsilon = self.epsilon\n",
    "\n",
    "        valid_actions = self.env.get_valid_actions()\n",
    "        if random.random() < epsilon:\n",
    "            action = random.choice(valid_actions)\n",
    "        else:\n",
    "            Q_s = self.q_table[state[0], state[1], valid_actions]\n",
    "            action = random.choice(valid_actions[np.flatnonzero(Q_s == np.max(Q_s))])\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************************************************************************************\n",
      "# up:      0.00 # up:      0.00 # up:      0.00 # up:      0.00 # up:      0.00 # up:      0.00 # up:      0.00 #\n",
      "# right:  79.07 # right:  80.88 # right:  82.70 # right:  84.55 # right:  86.41 # right:  88.30 # right:   0.00 #\n",
      "# down:   79.07 # down:   80.88 # down:   82.70 # down:   84.55 # down:   86.41 # down:   88.30 # down:   90.20 #\n",
      "# left:    0.00 # left:   77.28 # left:   79.07 # left:   80.88 # left:   82.70 # left:   84.55 # left:   86.41 #\n",
      "*****************************************************************************************************************\n",
      "# up:     77.28 # up:     79.07 # up:     80.88 # up:     82.70 # up:     84.55 # up:     86.41 # up:     88.30 #\n",
      "# right:  80.88 # right:  82.70 # right:  84.55 # right:  86.41 # right:  88.30 # right:  90.20 # right:   0.00 #\n",
      "# down:   80.88 # down:   82.70 # down:   84.55 # down:   86.41 # down:   88.30 # down:   90.20 # down:   92.12 #\n",
      "# left:    0.00 # left:   79.07 # left:   80.88 # left:   82.70 # left:   84.55 # left:   86.41 # left:   88.30 #\n",
      "*****************************************************************************************************************\n",
      "# up:     79.07 # up:     80.88 # up:     82.70 # up:     84.55 # up:     86.41 # up:     88.30 # up:     90.20 #\n",
      "# right:  82.70 # right:  84.55 # right:  86.41 # right:  88.30 # right:  90.20 # right:  92.12 # right:   0.00 #\n",
      "# down:   82.70 # down:   84.55 # down:   86.41 # down:   88.30 # down:   90.20 # down:   92.12 # down:   94.06 #\n",
      "# left:    0.00 # left:   80.88 # left:   82.70 # left:   84.55 # left:   86.41 # left:   88.30 # left:   90.20 #\n",
      "*****************************************************************************************************************\n",
      "# up:     80.88 # up:     82.70 # up:     84.55 # up:     86.41 # up:     88.30 # up:     90.20 # up:     92.12 #\n",
      "# right:  84.55 # right:  86.41 # right:  88.30 # right:  90.20 # right:  92.12 # right:  94.06 # right:   0.00 #\n",
      "# down:   84.55 # down:   86.41 # down:   88.30 # down:   90.20 # down:   92.12 # down:   94.06 # down:   96.02 #\n",
      "# left:    0.00 # left:   82.70 # left:   84.55 # left:   86.41 # left:   88.30 # left:   90.20 # left:   92.12 #\n",
      "*****************************************************************************************************************\n",
      "# up:     82.70 # up:     84.55 # up:     86.41 # up:     88.30 # up:     90.20 # up:     92.12 # up:     94.06 #\n",
      "# right:  86.41 # right:  88.30 # right:  90.20 # right:  92.12 # right:  94.06 # right:  96.02 # right:   0.00 #\n",
      "# down:   86.41 # down:   88.30 # down:   90.20 # down:   92.12 # down:   94.06 # down:   96.02 # down:   98.00 #\n",
      "# left:    0.00 # left:   84.55 # left:   86.41 # left:   88.30 # left:   90.20 # left:   92.12 # left:   94.06 #\n",
      "*****************************************************************************************************************\n",
      "# up:     84.55 # up:     86.41 # up:     88.30 # up:     90.20 # up:     92.12 # up:     94.06 # up:     96.02 #\n",
      "# right:  88.30 # right:  90.20 # right:  92.12 # right:  94.06 # right:  96.02 # right:  98.00 # right:   0.00 #\n",
      "# down:   84.55 # down:   86.41 # down:   88.30 # down:   90.20 # down:   92.12 # down:   89.00 # down:  100.00 #\n",
      "# left:    0.00 # left:   86.41 # left:   88.30 # left:   90.20 # left:   92.12 # left:   94.06 # left:   96.02 #\n",
      "*****************************************************************************************************************\n",
      "# up:     86.41 # up:     88.30 # up:     90.20 # up:     92.12 # up:     94.06 # up:     96.02 # up:      0.00 #\n",
      "# right:  86.41 # right:  88.30 # right:  90.20 # right:  92.12 # right:  89.00 # right: 100.00 # right:   0.00 #\n",
      "# down:    0.00 # down:    0.00 # down:    0.00 # down:    0.00 # down:    0.00 # down:    0.00 # down:    0.00 #\n",
      "# left:    0.00 # left:   84.55 # left:   86.41 # left:   88.30 # left:   90.20 # left:   92.12 # left:    0.00 #\n",
      "*****************************************************************************************************************\n",
      "num_episode=999\n"
     ]
    }
   ],
   "source": [
    "env = GridEnv()\n",
    "agent = Agent(env)\n",
    "\n",
    "for n_episode in range(NUM_EPISODE):\n",
    "    state = env.reset()\n",
    "\n",
    "    while True:\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "\n",
    "        agent.update_table(state, action, reward, next_state)\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    debug_str = \"\"\n",
    "    for h in range(env.height):\n",
    "        for w in range(env.width):\n",
    "            debug_str += '****************'\n",
    "        debug_str += \"*\\n\"\n",
    "        for w in range(env.width):\n",
    "            debug_str += '# up:' + str('%.2f ' % (agent.q_table[h, w, 0])).rjust(11)\n",
    "        debug_str += \"#\\n\"\n",
    "        for w in range(env.width):\n",
    "            debug_str += '# right:' + str('%.2f ' % (agent.q_table[h, w, 1])).rjust(8)\n",
    "        debug_str += \"#\\n\"\n",
    "        for w in range(env.width):\n",
    "            debug_str += '# down:' + str('%.2f ' % (agent.q_table[h, w, 2])).rjust(9)\n",
    "        debug_str += \"#\\n\"\n",
    "        for w in range(env.width):\n",
    "            debug_str += '# left:' + str('%.2f ' % (agent.q_table[h, w, 3])).rjust(9)\n",
    "        debug_str += \"#\\n\"\n",
    "    for c in range(env.width):\n",
    "        debug_str += '****************'\n",
    "    debug_str += \"*\\n\"\n",
    "    debug_str += \"num_episode=%d\" % n_episode\n",
    "\n",
    "#     os.system(\"clear\")\n",
    "    clear_output()\n",
    "    print(debug_str)\n",
    "\n",
    "# save table\n",
    "np.save(\"q_table.npy\", agent.q_table)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
