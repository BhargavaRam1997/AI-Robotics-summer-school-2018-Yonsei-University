{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPISODE = 1000\n",
    "\n",
    "N_HEIGHT = 7 # grid height\n",
    "N_WIDTH = 7 # grid width "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridEnv(object):\n",
    "    def __init__(self):\n",
    "        self.height = N_HEIGHT\n",
    "        self.width = N_WIDTH\n",
    "\n",
    "        self.action_dict = {\"up\":0, \"right\": 1, \"down\": 2, \"left\": 3}\n",
    "        self.action_coords = np.array([[-1,0], [0,1], [1,0], [0,-1]], dtype=np.int)\n",
    "        self.num_actions = len(self.action_dict.keys())\n",
    "\n",
    "        self.state_dim = (self.height, self.width)\n",
    "        self.action_dim = (self.num_actions,)\n",
    "        self.state_action_dim = self.state_dim + self.action_dim\n",
    "\n",
    "        self.obstacles = []\n",
    "        self.add_obstacle(6, 5)\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def add_obstacle(self, h, w):\n",
    "        self.obstacles.append([h, w])\n",
    "\n",
    "    def get_valid_actions(self):\n",
    "        actions = []\n",
    "        h = self.current_state[0]\n",
    "        w = self.current_state[1]\n",
    "        if (h > 0): actions.append(self.action_dict[\"up\"])\n",
    "        if (h < self.height-1): actions.append(self.action_dict[\"down\"])\n",
    "        if (w > 0): actions.append(self.action_dict[\"left\"])\n",
    "        if (w < self.width-1): actions.append(self.action_dict[\"right\"])\n",
    "        actions = np.array(actions, dtype=np.int)\n",
    "        return actions\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_state = np.array([0, 0], dtype=np.int)\n",
    "        return self.current_state\n",
    "\n",
    "    def step(self, action):\n",
    "        self.current_state = np.add(self.current_state, self.action_coords[action])\n",
    "        if np.array_equal(self.current_state, [self.height-1, self.width-1]):\n",
    "            reward = 100\n",
    "            done = True\n",
    "        elif list(self.current_state) in self.obstacles:\n",
    "            reward = -10\n",
    "            done = False\n",
    "        else:\n",
    "            reward = -1\n",
    "            done = False\n",
    "        return self.current_state, reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "\n",
    "        self.learning_rate = 0.1\n",
    "        self.discount_factor = 0.99 \n",
    "        self.epsilon = 0.9\n",
    "\n",
    "        self.q_table = np.zeros(env.state_action_dim, dtype=np.float)\n",
    "\n",
    "    def update_table(self, state, action, reward, next_state, next_action):\n",
    "        q_prev = self.q_table[state[0], state[1], action]\n",
    "        q_target = reward + self.discount_factor * self.q_table[next_state[0], next_state[1], next_action]\n",
    "        self.q_table[state[0], state[1], action] += self.learning_rate * (q_target - q_prev)\n",
    "\n",
    "    def get_action(self, state, greedy=False):\n",
    "        if greedy:\n",
    "            epsilon = 0\n",
    "        else:\n",
    "            epsilon = self.epsilon\n",
    "\n",
    "        valid_actions = self.env.get_valid_actions()\n",
    "        if random.random() < epsilon:\n",
    "            action = random.choice(valid_actions)\n",
    "        else:\n",
    "            Q_s = self.q_table[state[0], state[1], valid_actions]\n",
    "            action = random.choice(valid_actions[np.flatnonzero(Q_s == np.max(Q_s))])\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************************************************************************************************************\n",
      "# up:      0.00 # up:      0.00 # up:      0.00 # up:      0.00 # up:      0.00 # up:      0.00 # up:      0.00 #\n",
      "# right: -17.75 # right: -15.11 # right: -11.59 # right:  -9.37 # right:  -6.00 # right:  -5.13 # right:   0.00 #\n",
      "# down:  -17.97 # down:  -16.82 # down:  -12.99 # down:   -6.77 # down:   -5.09 # down:   -2.11 # down:   -2.33 #\n",
      "# left:    0.00 # left:  -18.64 # left:  -17.52 # left:  -15.71 # left:  -13.33 # left:   -8.13 # left:   -6.03 #\n",
      "*****************************************************************************************************************\n",
      "# up:    -18.73 # up:    -17.43 # up:    -14.30 # up:    -11.42 # up:     -9.49 # up:     -5.09 # up:     -4.31 #\n",
      "# right: -16.90 # right: -15.05 # right:  -9.04 # right:  -7.86 # right:   0.53 # right:   0.00 # right:   0.00 #\n",
      "# down:  -16.11 # down:  -15.90 # down:  -10.48 # down:   -2.87 # down:    2.95 # down:    7.20 # down:   12.45 #\n",
      "# left:    0.00 # left:  -17.65 # left:  -16.55 # left:  -13.98 # left:   -9.41 # left:   -5.02 # left:   -3.80 #\n",
      "*****************************************************************************************************************\n",
      "# up:    -17.93 # up:    -16.93 # up:    -12.92 # up:     -7.76 # up:     -5.85 # up:      0.63 # up:      0.87 #\n",
      "# right: -14.66 # right:  -9.58 # right:  -0.57 # right:   2.89 # right:   9.70 # right:  11.19 # right:   0.00 #\n",
      "# down:  -15.11 # down:  -11.45 # down:   -6.99 # down:   -1.18 # down:   11.68 # down:   18.83 # down:   26.56 #\n",
      "# left:    0.00 # left:  -16.66 # left:  -15.16 # left:   -6.09 # left:   -4.79 # left:    0.96 # left:    9.11 #\n",
      "*****************************************************************************************************************\n",
      "# up:    -16.62 # up:    -13.91 # up:     -6.41 # up:     -5.42 # up:      0.19 # up:      8.12 # up:     13.76 #\n",
      "# right: -12.32 # right:  -6.17 # right:  -0.03 # right:   9.50 # right:  20.19 # right:  27.87 # right:   0.00 #\n",
      "# down:  -12.72 # down:   -8.67 # down:   -1.58 # down:    6.51 # down:   14.59 # down:   28.56 # down:   47.37 #\n",
      "# left:    0.00 # left:  -14.39 # left:  -12.15 # left:   -6.70 # left:    1.79 # left:    8.97 # left:   18.39 #\n",
      "*****************************************************************************************************************\n",
      "# up:    -15.11 # up:    -11.97 # up:     -5.00 # up:      0.21 # up:      4.60 # up:     19.89 # up:     21.52 #\n",
      "# right:  -8.65 # right:  -3.63 # right:   4.31 # right:  17.32 # right:  24.77 # right:  40.70 # right:   0.00 #\n",
      "# down:   -9.36 # down:   -6.37 # down:    1.86 # down:   12.68 # down:   31.28 # down:   46.73 # down:   67.69 #\n",
      "# left:    0.00 # left:  -11.96 # left:   -9.28 # left:   -2.53 # left:    2.09 # left:   19.92 # left:   27.99 #\n",
      "*****************************************************************************************************************\n",
      "# up:    -12.21 # up:     -9.04 # up:     -3.13 # up:      7.09 # up:     14.03 # up:     32.66 # up:     41.25 #\n",
      "# right:  -5.97 # right:  -0.16 # right:  16.71 # right:  26.26 # right:  42.48 # right:  57.63 # right:   0.00 #\n",
      "# down:   -7.32 # down:   -2.29 # down:    3.25 # down:   14.67 # down:   32.13 # down:   61.27 # down:  100.00 #\n",
      "# left:    0.00 # left:   -9.44 # left:   -5.17 # left:    1.28 # left:   18.06 # left:   29.47 # left:   50.33 #\n",
      "*****************************************************************************************************************\n",
      "# up:    -10.44 # up:     -5.04 # up:      3.07 # up:     13.10 # up:     21.65 # up:     48.89 # up:      0.00 #\n",
      "# right:  -3.57 # right:   5.36 # right:  19.71 # right:  35.81 # right:  45.64 # right: 100.00 # right:   0.00 #\n",
      "# down:    0.00 # down:    0.00 # down:    0.00 # down:    0.00 # down:    0.00 # down:    0.00 # down:    0.00 #\n",
      "# left:    0.00 # left:   -6.67 # left:   -1.89 # left:    5.47 # left:   21.99 # left:   33.85 # left:    0.00 #\n",
      "*****************************************************************************************************************\n",
      "num_episode=999\n"
     ]
    }
   ],
   "source": [
    "env = GridEnv()\n",
    "agent = Agent(env)\n",
    "\n",
    "for n_episode in range(NUM_EPISODE):\n",
    "    state = env.reset()\n",
    "\n",
    "    while True:\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        next_action = agent.get_action(next_state)\n",
    "\n",
    "        agent.update_table(state, action, reward, next_state, next_action)\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    debug_str = \"\"\n",
    "    for h in range(env.height):\n",
    "        for w in range(env.width):\n",
    "            debug_str += '****************'\n",
    "        debug_str += \"*\\n\"\n",
    "        for w in range(env.width):\n",
    "            debug_str += '# up:' + str('%.2f ' % (agent.q_table[h, w, 0])).rjust(11)\n",
    "        debug_str += \"#\\n\" \n",
    "        for w in range(env.width):\n",
    "            debug_str += '# right:' + str('%.2f ' % (agent.q_table[h, w, 1])).rjust(8)\n",
    "        debug_str += \"#\\n\"\n",
    "        for w in range(env.width):\n",
    "            debug_str += '# down:' + str('%.2f ' % (agent.q_table[h, w, 2])).rjust(9)\n",
    "        debug_str += \"#\\n\"\n",
    "        for w in range(env.width):\n",
    "            debug_str += '# left:' + str('%.2f ' % (agent.q_table[h, w, 3])).rjust(9)\n",
    "        debug_str += \"#\\n\"\n",
    "    for c in range(env.width):\n",
    "        debug_str += '****************'\n",
    "    debug_str += \"*\\n\"\n",
    "    debug_str += \"num_episode=%d\" % n_episode\n",
    "\n",
    "#     os.system(\"clear\")\n",
    "    clear_output()\n",
    "    print(debug_str)\n",
    "\n",
    "# save table\n",
    "np.save(\"q_table.npy\", agent.q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
